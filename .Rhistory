with(phase.loadings[PHASE_FOR_OQ==1,], points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=cols[REGION]))
legend("topright", legend="B: c.6100-5500 cal. BC", cex=1.1, bty="n")
with(taxon.loadings, plot(Dim1, Dim2, pch=4, col=bi.col[black], xlab="Dimension 1", ylab="Dimension 2", xlim=xlims, ylim=ylims, cex.lab=0.9, cex.axis=0.9))
with(phase.loadings[PHASE_FOR_OQ==2], points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=cols[REGION]))
legend("topright", legend="C: c.5500-5000 cal. BC", cex=1.1, bty="n")
with(taxon.loadings, plot(Dim1, Dim2, pch=4, col=bi.col[black], xlab="Dimension 1", ylab="", xlim=xlims, ylim=ylims, cex.lab=0.9, cex.axis=0.9))
with(phase.loadings[PHASE_FOR_OQ==3], points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=cols[REGION]))
legend("topright", legend="D: c.5000-4500 cal. BC", cex=1.1, bty="n")
col.codes <- col.codes[order(order)]
legend.cols <- rgb(col.codes$red, col.codes$green, col.codes$blue, col.codes$alpha, names=col.codes$Region, maxColorValue = 255)
legend("bottomright", pch=21, cex=0.9, pt.bg=legend.cols, legend=col.codes$Region)
#CA by recovery
xlims <- c(min(taxon.loadings$Dim1, ca.results$Dim1)-0.35, max(taxon.loadings$Dim1, ca.results$Dim1)+0.1)
ylims <- c(min(taxon.loadings$Dim2, ca.results$Dim2)-0.1, max(taxon.loadings$Dim2, ca.results$Dim2)+0.1)
with(taxon.loadings, plot(Dim1, Dim2, pch=4, xlab="Dimension 1", ylab="Dimension 2", col=bi.col[black], xlim=xlims, ylim=ylims, cex.lab=0.9, cex.axis=0.9))
with(taxon.loadings, text(Dim1, Dim2, labels=genus, cex=1, col=bi.col[black], font=3, pos=label.pos))
with(phase.loadings, points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=sieve.cols[RECOVERY_GROUP+1]))
legend("topright", pch=21, pt.bg=sieve.cols, legend=c("No sieving", "Limited sieving", "Frequent sieving", "100% sieved", "Unknown"))
#CA by recovery
with(taxon.loadings, plot(Dim1, Dim2, pch=4, xlab="Dimension 1", ylab="Dimension 2", col=bi.col[black], xlim=xlims, ylim=ylims, cex.lab=0.9, cex.axis=0.9))
with(taxon.loadings, text(Dim1, Dim2, labels=genus, cex=1, col=bi.col[black], font=3, pos=label.pos))
with(phase.loadings, points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=sieve.cols[RECOVERY_GROUP+1]))
legend("topright", pch=21, pt.bg=sieve.cols, legend=c("No sieving", "Limited sieving", "Frequent sieving", "100% sieved", "Unknown"))
par(mfrow=c(1,1))
#CA by recovery
with(taxon.loadings, plot(Dim1, Dim2, pch=4, xlab="Dimension 1", ylab="Dimension 2", col=bi.col[black], xlim=xlims, ylim=ylims, cex.lab=0.9, cex.axis=0.9))
with(taxon.loadings, text(Dim1, Dim2, labels=genus, cex=1, col=bi.col[black], font=3, pos=label.pos))
with(phase.loadings, points(Dim1, Dim2, pch=type.symbols[TYPE], cex=1.2, bg=sieve.cols[RECOVERY_GROUP+1]))
legend("topright", pch=21, pt.bg=sieve.cols, legend=c("No sieving", "Limited sieving", "Frequent sieving", "100% sieved", "Unknown"))
library(devtools)
install.packages("devtools")
install.packages("gert")
install.packages("gert")
install.packages("devtools")
library(devtools)
install_github("davidcorton/archSeries")
library(data.table)
data <- data("context")
View(context)
rm(context)
data("period")
data <- period
data("period")
data <- period
probs <- 1
values=1
ds.fun=sum
real=TRUE
dummy=FALSE
comp.field=NULL
comp.values=NULL
context.fields=c("ID")
quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)
start.date=NULL
end.date=NULL
a=1
b=1
bin.width=100
reps=100
RoC=NULL
summ=TRUE
End <- Start <- sim <- bin <- .SD <- bin.no <- dummy.bin <- NULL
data <- data.table(cbind(data, values, a, b, uniqueID=1:nrow(data))) #appends weights and beta distribution parameters to list of date ranges, recycling if necessary.
View(data)
if(is.null(start.date)) {
start.date <- min(data$Start)
}
if(is.null(end.date)) {
end.date <- max(data$End)
}
data <- data[End >= start.date & Start <= end.date]  #drops records outside the date range FROM BOTH SIMULATION SETS
View(data)
View(period)
UoA <- data[, j=list(n=length(uniqueID)), by=c(context.fields, "Start", "End", "a", "b")]
#Create table of unique units of analysis (e.g. contexts) for simulation
UoA <- data[, j=list(n=length(`uniqueID`)), by=c(context.fields, "Start", "End", "a", "b")]
View(data)
#Create table of unique units of analysis (e.g. contexts) for simulation
UoA <- data[, j = list( n = length(Start) ), by = c(context.fields, "Start", "End", "a", "b")]
End <- Start <- sim <- bin <- .SD <- bin.no <- dummy.bin <- NULL
data <- data.table(cbind(data, values, a, b, uniqueID=1:nrow(data))) #appends weights and beta distribution parameters to list of date ranges, recycling if necessary.
if(is.null(start.date)) {
start.date <- min(data$Start)
}
if(is.null(end.date)) {
end.date <- max(data$End)
}
data <- data[End >= start.date & Start <= end.date]  #drops records outside the date range FROM BOTH SIMULATION SETS
UoA <- data[, j = list( n = length(uniqueID) ), by = c(context.fields, "Start", "End", "a", "b")]
data("period")
data <- period
probs <- 1
values=1
ds.fun=sum
real=TRUE
dummy=FALSE
comp.field=NULL
comp.values=NULL
context.fields=NULL
quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)
start.date=NULL
end.date=NULL
a=1
b=1
bin.width=100
reps=100
RoC=NULL
summ=TRUE
End <- Start <- sim <- bin <- .SD <- bin.no <- dummy.bin <- NULL
#Tidy up input data
data <- data.table(cbind(data, values, a, b, uniqueID=1:nrow(data))) #appends weights and beta distribution parameters to list of date ranges, recycling if necessary.
#Read start and end dates from input data if not specified
if(is.null(start.date)) {
start.date <- min(data$Start)
}
if(is.null(end.date)) {
end.date <- max(data$End)
}
data <- data[End >= start.date & Start <= end.date]  #drops records outside the date range FROM BOTH SIMULATION SETS
UoA <- data[, j = list( n = length(uniqueID) ), by = c(context.fields, "Start", "End", "a", "b")]
View(UoA)
context.fields="SITE_P"
data("period")
data <- period
probs <- 1
values=1
ds.fun=sum
real=TRUE
dummy=FALSE
comp.field=NULL
comp.values=NULL
context.fields="SITE_P"
quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)
start.date=NULL
end.date=NULL
a=1
b=1
bin.width=100
reps=100
RoC=NULL
summ=TRUE
End <- Start <- sim <- bin <- .SD <- bin.no <- dummy.bin <- NULL
#Tidy up input data
data <- data.table(cbind(data, values, a, b, uniqueID=1:nrow(data))) #appends weights and beta distribution parameters to list of date ranges, recycling if necessary.
#Read start and end dates from input data if not specified
if(is.null(start.date)) {
start.date <- min(data$Start)
}
if(is.null(end.date)) {
end.date <- max(data$End)
}
data <- data[End >= start.date & Start <= end.date]  #drops records outside the date range FROM BOTH SIMULATION SETS
#Create table of unique units of analysis (e.g. contexts) for simulation
UoA <- data[, j = list( n = length(uniqueID) ), by = c(context.fields, "Start", "End", "a", "b")]
View(UoA)
# Separate comparison groups, if relevant
if(!is.null(comp.field)) {
if(is.null(comp.values)) {comp.values <- unique(data[!get(comp.field)=="" & !is.na(get(comp.field)), get(comp.field)])} #if necessary, define comp.values as all unique values of comp.field
data <- data[get(comp.field) %in% comp.values]   #filter out categories not to be compared, if any
x <- as.formula(paste(paste(context.fields, collapse="+"), "+Start+End+a+b+uniqueID~", comp.field, sep=""))
data <- dcast.data.table(data, formula=x, value.var="values", fill=0)
}
if(is.vector(probs)==TRUE & length(probs)>1) {bin.width <- (end.date - start.date) / length(probs)}  #if probs supplied, use to set bin.widths
if(sum(class(probs)=="data.frame")==1) {bin.width <- (end.date - start.date) / nrow(probs)}  #likewise if supplied as data.frame/data.table
breaks <- seq(start.date, end.date, bin.width) #sets breaks
labels <- numeric(0)
for(i in 1:(length(breaks) - 1)) {
labels[i] <- paste(breaks[i], breaks[i + 1], sep="-") #sets bin labels
}
probs <- cbind(1:length(labels), probs) #recycles probs to length of labels, if necessary
rep.no <- rep(1:reps, each = nrow(UoA))
UoA <- cbind(rep.no, UoA) #recycles UoA table 'reps' times to provide frame for simulation
#Set up main data table for simulation results  # SHOULD REDUCE TO JUST NEEDED FIELDS!
rep.no <- rep(1:reps, each = nrow(data))
data <- cbind(rep.no, data) #recycles input data 'reps' times to provide frame for simulation
results <- data.table(rep.no=rep(1:reps, each=length(labels)), bin.no=rep(1:length(labels), reps), bin=rep(labels, reps))
if(real==TRUE) {
UoA[, sim := {x <- rbeta(nrow(UoA), abs(UoA$a), abs(UoA$b)); (x * (UoA$End - UoA$Start)) + UoA$Start}] #simulates a date for each row
UoA[, bin := cut(sim, breaks, labels=labels)] #finds the relevant bin for each simulated date
}
if(dummy==TRUE) {
UoA[, dummy.bin := sample(labels, size=nrow(UoA), replace=TRUE, prob=probs[, 2])]
}
data <- merge(data, UoA, by=c("rep.no", context.fields, "Start", "End", "a", "b"))
if(real==TRUE) {
if(is.null(comp.field)) {
real <- data[is.na(bin)==FALSE, j=list(value=ds.fun(as.numeric(values)), n=length(uniqueID)), by=list(rep.no, bin)] #applies ds.fun to values by bin and rep number
} else {
real <- data[is.na(bin)==FALSE, lapply(.SD, ds.fun), by=list(rep.no, bin), .SDcols=as.character(comp.values)] #applies ds.fun to values by bin and rep number
setnames(real, old=as.character(comp.values), new=paste(comp.values, ".value", sep=""))
}
#Merge with results frame
results <- merge(results, real, by=c("rep.no", "bin"), all=TRUE)
}
if(dummy==TRUE) {
if(is.null(comp.field)) {
dummy <- data[is.na(bin)==FALSE, j=list(dummy=ds.fun(as.numeric(values))), by=list(rep.no, dummy.bin)] #sums weights by bin and rep number
} else {
dummy <- data[is.na(bin)==FALSE, lapply(.SD, ds.fun), by=list(rep.no, dummy.bin), .SDcols=as.character(comp.values)] #sums weights by bin and rep number
setnames(dummy, old=as.character(comp.values), new=paste(comp.values, ".dummy", sep=""))
}
#Merge with results frame
setnames(dummy, "dummy.bin", "bin")
results <- merge(results, dummy, by=c("rep.no", "bin"), all=TRUE)
}
#Replace any missing values with 0
results[is.na(results)] <- 0
results <- results[order(rep.no, bin.no)]
#Calculate rate of change variables
if(!is.null(RoC)) {results <- RoC.fun(results, type=RoC)}
#Create summary dataset, if required and return results
if(summ==TRUE) {
summary <- sim.summ(results)
results <- list(full=results, summary=summary)
}
#' Summarise basic output from simulation functions.
#'
#' A utility function designed to be used within the simulation functions in this package, but which can also be used alone if useful.
#' @param results A data table resembling the full-form output from date.simulate or a related function.
#' @param summ.col Character: which columns(s) in 'results' should be summarised? Defaults to NULL, in which case all columns not called
#'      "bin", "bin.no" or "rep.no" are summarised.
#' @param quant.list Numeric vector of quantiles to be calculated in a summary table. Defaults to c(0.025,0.25,0.5,0.75,0.975).
#' @return A long-format data table with four named columns: 'bin', character specifying chronological bin in terms of date range; 'V1',
#'      the relevant value for the given bin at a given quantile; 'quantile', the quantile at which V1 is calculated; 'id', character
#'      specifying which column from 'results' V1 is based on: potentially "count", "dummy", "RoC.count", or "RoC.dummy" depending on the
#'      function call which created 'results'.
#' @export
#' @examples
#' date.ranges <- data.table(Start=c(450, 450, 600), End=c(700, 800, 650), frag.count=c(3, 6, 25))
#' x <- date.simulate(date.ranges, weight=date.ranges$frag.count, bin.width=50, reps=200, summ=FALSE)
#' x.summary <- sim.summ(x)
sim.summ <- function(results, summ.col=NULL, quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)) {
bin <- id <- NULL
if(is.null(summ.col)==TRUE) {summ.col <- colnames(results)[!colnames(results) %in% c("rep.no", "bin", "bin.no")]}
#Create summary tables
for(i in 1:length(summ.col)) {
x <- results[, quantile(get(summ.col[i]), probs=quant.list, na.rm=TRUE), by=bin] #calculate quantiles
x[, quantile := quant.list] #create column to specify quantiles
x[, id := summ.col[i]] #create column to specify variable
if(i==1) {summary <- data.table()}
summary <- rbind(summary, x)
}
#Return summary table
summary
}
if(summ==TRUE) {
summary <- sim.summ(results)
results <- list(full=results, summary=summary)
}
summ.col=NULL
bin <- id <- NULL
if(is.null(summ.col)==TRUE) {summ.col <- colnames(results)[!colnames(results) %in% c("rep.no", "bin", "bin.no")]}
i=1
x <- results[, quantile(get(summ.col[i]), probs=quant.list, na.rm=TRUE), by=bin] #calculate quantiles
View(x)
View(results)
x[, quantile := quant.list] #create column to specify quantiles
x[, quantile := rep(quant.list)] #create column to specify quantiles
x[, quantile := rep(quant.list)] #create column to specify quantiles
?rep()
x[, quantile := rep(quant.list, length.out = nrow(x))] #create column to specify quantiles
x[, id := summ.col[i]] #create column to specify variable
if(i==1) {summary <- data.table()}
summary <- rbind(summary, x)
i=2
x <- results[, quantile(get(summ.col[i]), probs=quant.list, na.rm=TRUE), by=bin] #calculate quantiles
x[, quantile := rep(quant.list, length.out = nrow(x))] #create column to specify quantiles
x[, id := summ.col[i]] #create column to specify variable
if(i==1) {summary <- data.table()}
summary <- rbind(summary, x)
View(x)
#' Summarise basic output from simulation functions.
#'
#' A utility function designed to be used within the simulation functions in this package, but which can also be used alone if useful.
#' @param results A data table resembling the full-form output from date.simulate or a related function.
#' @param summ.col Character: which columns(s) in 'results' should be summarised? Defaults to NULL, in which case all columns not called
#'      "bin", "bin.no" or "rep.no" are summarised.
#' @param quant.list Numeric vector of quantiles to be calculated in a summary table. Defaults to c(0.025,0.25,0.5,0.75,0.975).
#' @return A long-format data table with four named columns: 'bin', character specifying chronological bin in terms of date range; 'V1',
#'      the relevant value for the given bin at a given quantile; 'quantile', the quantile at which V1 is calculated; 'id', character
#'      specifying which column from 'results' V1 is based on: potentially "count", "dummy", "RoC.count", or "RoC.dummy" depending on the
#'      function call which created 'results'.
#' @export
#' @examples
#' date.ranges <- data.table(Start=c(450, 450, 600), End=c(700, 800, 650), frag.count=c(3, 6, 25))
#' x <- date.simulate(date.ranges, weight=date.ranges$frag.count, bin.width=50, reps=200, summ=FALSE)
#' x.summary <- sim.summ(x)
sim.summ <- function(results, summ.col=NULL, quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)) {
bin <- id <- NULL
if(is.null(summ.col)==TRUE) {summ.col <- colnames(results)[!colnames(results) %in% c("rep.no", "bin", "bin.no")]}
#Create summary tables
for(i in 1:length(summ.col)) {
x <- results[, quantile(get(summ.col[i]), probs=quant.list, na.rm=TRUE), by=bin] #calculate quantiles
x[, quantile := rep(quant.list, length.out = nrow(x))] #create column to specify quantiles
x[, id := summ.col[i]] #create column to specify variable
if(i==1) {summary <- data.table()}
summary <- rbind(summary, x)
}
#Return summary table
summary
}
data("period")
data <- period
probs <- 1
values=1
ds.fun=sum
real=TRUE
dummy=FALSE
comp.field=NULL
comp.values=NULL
context.fields="SITE_P"
quant.list=c(0.025, 0.25, 0.5, 0.75, 0.975)
start.date=NULL
end.date=NULL
a=1
b=1
bin.width=100
reps=100
RoC=NULL
summ=TRUE
summ.col=NULL
End <- Start <- sim <- bin <- .SD <- bin.no <- dummy.bin <- NULL
data <- data.table(cbind(data, values, a, b, uniqueID=1:nrow(data))) #appends weights and beta distribution parameters to list of date ranges, recycling if necessary.
#Read start and end dates from input data if not specified
if(is.null(start.date)) {
start.date <- min(data$Start)
}
if(is.null(end.date)) {
end.date <- max(data$End)
}
data <- data[End >= start.date & Start <= end.date]  #drops records outside the date range FROM BOTH SIMULATION SETS
#Create table of unique units of analysis (e.g. contexts) for simulation
UoA <- data[, j = list( n = length(uniqueID) ), by = c(context.fields, "Start", "End", "a", "b")]
if(!is.null(comp.field)) {
if(is.null(comp.values)) {comp.values <- unique(data[!get(comp.field)=="" & !is.na(get(comp.field)), get(comp.field)])} #if necessary, define comp.values as all unique values of comp.field
data <- data[get(comp.field) %in% comp.values]   #filter out categories not to be compared, if any
x <- as.formula(paste(paste(context.fields, collapse="+"), "+Start+End+a+b+uniqueID~", comp.field, sep=""))
data <- dcast.data.table(data, formula=x, value.var="values", fill=0)
}
#Reset bin.width based on probs, if necessary
if(is.vector(probs)==TRUE & length(probs)>1) {bin.width <- (end.date - start.date) / length(probs)}  #if probs supplied, use to set bin.widths
if(sum(class(probs)=="data.frame")==1) {bin.width <- (end.date - start.date) / nrow(probs)}  #likewise if supplied as data.frame/data.table
#Set up breaks and labels
breaks <- seq(start.date, end.date, bin.width) #sets breaks
labels <- numeric(0)
for(i in 1:(length(breaks) - 1)) {
labels[i] <- paste(breaks[i], breaks[i + 1], sep="-") #sets bin labels
}
probs <- cbind(1:length(labels), probs) #recycles probs to length of labels, if necessary
#Set up UoA table for simulation
rep.no <- rep(1:reps, each = nrow(UoA))
UoA <- cbind(rep.no, UoA) #recycles UoA table 'reps' times to provide frame for simulation
#Set up main data table for simulation results  # SHOULD REDUCE TO JUST NEEDED FIELDS!
rep.no <- rep(1:reps, each = nrow(data))
data <- cbind(rep.no, data) #recycles input data 'reps' times to provide frame for simulation
results <- data.table(rep.no=rep(1:reps, each=length(labels)), bin.no=rep(1:length(labels), reps), bin=rep(labels, reps))
#Simulate real and/or dummy data
if(real==TRUE) {
UoA[, sim := {x <- rbeta(nrow(UoA), abs(UoA$a), abs(UoA$b)); (x * (UoA$End - UoA$Start)) + UoA$Start}] #simulates a date for each row
UoA[, bin := cut(sim, breaks, labels=labels)] #finds the relevant bin for each simulated date
}
if(dummy==TRUE) {
UoA[, dummy.bin := sample(labels, size=nrow(UoA), replace=TRUE, prob=probs[, 2])]
}
#Merge results back into full data
data <- merge(data, UoA, by=c("rep.no", context.fields, "Start", "End", "a", "b"))
#Aggregate real and/or dummy data by bin
if(real==TRUE) {
if(is.null(comp.field)) {
real <- data[is.na(bin)==FALSE, j=list(value=ds.fun(as.numeric(values)), n=length(uniqueID)), by=list(rep.no, bin)] #applies ds.fun to values by bin and rep number
} else {
real <- data[is.na(bin)==FALSE, lapply(.SD, ds.fun), by=list(rep.no, bin), .SDcols=as.character(comp.values)] #applies ds.fun to values by bin and rep number
setnames(real, old=as.character(comp.values), new=paste(comp.values, ".value", sep=""))
}
#Merge with results frame
results <- merge(results, real, by=c("rep.no", "bin"), all=TRUE)
}
if(dummy==TRUE) {
if(is.null(comp.field)) {
dummy <- data[is.na(bin)==FALSE, j=list(dummy=ds.fun(as.numeric(values))), by=list(rep.no, dummy.bin)] #sums weights by bin and rep number
} else {
dummy <- data[is.na(bin)==FALSE, lapply(.SD, ds.fun), by=list(rep.no, dummy.bin), .SDcols=as.character(comp.values)] #sums weights by bin and rep number
setnames(dummy, old=as.character(comp.values), new=paste(comp.values, ".dummy", sep=""))
}
#Merge with results frame
setnames(dummy, "dummy.bin", "bin")
results <- merge(results, dummy, by=c("rep.no", "bin"), all=TRUE)
}
#Replace any missing values with 0
results[is.na(results)] <- 0
results <- results[order(rep.no, bin.no)]
#Calculate rate of change variables
if(!is.null(RoC)) {results <- RoC.fun(results, type=RoC)}
#Create summary dataset, if required and return results
if(summ==TRUE) {
summary <- sim.summ(results, quant.list = quant.list)
results <- list(full=results, summary=summary)
}
#Return results
results
#' Plot medians and confidence zones for simulation results.
#'
#' Plots defined confidence intervals (as polygons) and medians (as lines) for output from date.simulate or an associated function.
#' @param results A list resembling the output from date.simulate or a related function, or a data table resembling the second component
#'      thereof - i.e. the summary simulation results.
#' @param field.list A character vector of values in results$id to be plotted. Defaults to NULL, in which case all suitable
#'      values are used.
#' @param quant Numeric vector of length 2: lower and upper quantiles to define confidence zone. Defaults to c(0.025, 0.975), i.e. 95pc.
#' @param col.list Character vector: colours to be used for each column plotted. Defaults to c("darkred", "darkgreen", "blue", "grey",
#'      "goldenrod"). Nb. if more than five groups are plotted then the default colours will start to recycle.
#' @param opacity Numeric: opacity of each line. Defaults to 80.
#' @param ylim Numeric: an easy way to override the built-in scaling in plot - if a vector of length 1 is passed it will
#'      be converted into c(0, ylim) to be passed to the ylim argument in plot. Alternatively a vector of length 2 will be passed
#'      straight to plot as is. Defaults to NULL, in which case the built-in scaling in plot takes over.
#' @param med.line Logical: should a median line be plotted on top of eachconfidence polygon? Defaults to TRUE.
#' @param border Character: what type of border should the polygons have? Defaults to NA, i.e. no border.
#' @param small.n Character vector of colours to be used for boxes marking periods of low "effort", when plotting cpue results. Has no
#'      effect if 'results' doesn't have a third element called "small.n". Defaults to NULL, in which case no boxes are plotted.
#' @param small.n.op Numeric vector of length equal to 'small.n' (or otherwise recycled to that length) specifying opacity for small.n
#'      boxes. Defaults to 126.
#' @param add Logical: should data be added to current plot, or should axis.setup be called to start a new plot? Defaults to FALSE.
#' @param legend Logical: should an automatic legend of column names and corresponding colours be plotted? Defaults to TRUE.
#' @param ... Other graphical arguments to be passed to plot. Nb. (a) includes special arguments for axis.setup (currently just 'lab.sp'),
#'      (b) 'ylab' will default to "Estimated frequency density", as per axis.setup, unless specified here.
#' @return None.
#' @export
#' @examples
#' date.ranges <- data.table(Start=c(450, 450, 600), End=c(700, 800, 650), frags=c(3, 6, 25))
#' x <- freq.simulate(date.ranges, weight=date.ranges$frags, bin.width=50, reps=200)
#' poly.chron(x, field.list=c("dummy", "count"))
#'
poly.chron <- function(results, field.list=NULL, quant=c(0.025, 0.975), col.list=c("darkred", "darkgreen", "blue", "grey", "goldenrod"),
opacity=80, ylim=NULL, med.line=TRUE, border=NA, small.n=NULL, small.n.op=126, add=FALSE, legend=TRUE, ...) {
id <- V1 <- boxes <- NULL
if(class(results)[1]=="list") {
if(length(results)==3) {boxes <- results$small.n}
results <- results[[2]]
}
if(is.null(field.list)==TRUE) {field.list <- unique(results$id[!results$id == "n"])}
if(add==FALSE) {axis.setup(results, field.list=field.list, ylim=ylim, type=2, ...)}
if(!is.null(small.n) & !is.null(boxes)) {grey.zones(boxes, small.n, small.n.op, ylim[length(ylim)])} #Sets up boxes to highlight small n
plist <- data.frame(field.list, col.list[1:length(field.list)], opacity)
a <- col2rgb(plist[,2])
b <- character()
for(i in 1:nrow(plist)) {
b[i] <- rgb(a[1, i], a[2, i], a[3, i], plist[i, 3], maxColorValue=255)
}
x <- c(1:length(unique(results$bin)), length(unique(results$bin)):1)
for(i in 1:length(field.list)) {
y <- c(results[id==field.list[i] & quantile==quant[1], V1], rev(results[id==field.list[i] & quantile==quant[2], V1]))
skip <- length(x) + 1
if(substr(field.list[i], 1, 3)=="RoC") {skip <- length(unique(results$bin))}
polygon(x[!x==skip], y[!x==skip], col=b[i], border=border)
if(med.line==TRUE) {
with(results[id==field.list[i] & quantile==0.500], lines(1:length(unique(bin)), V1, col=col.list[i], lwd=3))
with(results[id==field.list[i] & quantile==0.500], points(1:length(unique(bin)), V1, col=col.list[i], pch=19))
}
}
if(legend==TRUE) {with(results, legend("topright", legend=field.list, fill=b, bty="n"))}
}
plo.chron(results)
poly.chron(results)
#' Set up axes for archSeries plotting functions.
#'
#' A utility function designed to be used within the various plot functions in this package, but which can also be used alone to set up
#'      axes based on simulation data prior to plotting anything.
#' @param results A list resembling the output from date.simulate or a related function, or a data table resembling one of the components
#'      thereof.
#' @param field.list A character vector of names of variables that are intended to be plotted. Depending on 'results' and 'type' this may
#'      mean column names in a full simulation output or values of 'id' in a summary table. Defaults to NULL, in which case all suitable
#'      variables in 'results' are used. Nb. variables entitled "catch" or "effort" will be ignored, due to their roles in output from
#'      cpue.
#' @param lab.sp Integer: intervals at which to place bin labels. Defaults to 1, i.e. labelling every bin.
#' @param ylab Character: label for the y-axis. Defaults to "Estimated frequency density" (which is the only reason for making it a
#'      formal argument here rather than just allowing it to be passed to plot via ...).
#' @param xlab Character: label for the x-axis. Defaults to "".
#' @param ylim Numeric: an easy way to override the built-in scaling in plot - if a vector of length 1 is passed it will
#'      be converted into c(0, ylim) to be passed to the ylim argument in plot. Alternatively a vector of length 2 will be passed
#'      straight to plot as is. Defaults to NULL, in which case the built-in scaling in plot takes over.
#' @param type Integer: the type of simulation output to be plotted. 1 = full results (for lines.chron or box.chron), 2 = summary results
#'      (for poly.chron).
#' @param axis.lab Logical. Should labels be plotted for the x axis?
#' @param ... other graphical arguments to be passed to plot.
#' @return None.
#' @export
#' @examples
#' date.ranges <- data.table(Start=c(450, 450, 600), End=c(700, 800, 650), frags=c(3, 6, 25))
#' x <- date.simulate(date.ranges, weight=date.ranges$frags, context.fields=NULL, summ=FALSE)
#' axis.setup(x, lab.sp=2, type=1)
axis.setup <- function(results, field.list=NULL, lab.sp=1, ylab="", xlab="", ylim=NULL,
type=1, axis.lab=TRUE,...) {
id <- V1 <- NULL
if(class(results)[1]=="list") {results <- results[[type]]}
minmaxer <- numeric(0)
if(type==1) {
if(is.null(field.list)) {field.list <- colnames(results)[!colnames(results) %in% c("bin", "bin.no", "rep.no", "catch", "effort")]}
for(i in 1:length(field.list)) {minmaxer <- c(minmaxer, results[, get(field.list[i])])}
minmaxer <- cbind(minmaxer, unique(results$bin.no))
} else {
if(is.null(field.list)) {field.list <- unique(results$id)}
minmaxer <- as.matrix(cbind(results[id %in% field.list, V1], 1:length(unique(results$bin))))
}
if(!is.null(ylim)) {
if(length(ylim)==1) {ylim <- c(0, ylim)}
}
plot(minmaxer[, 2], minmaxer[, 1], xlab=xlab, xaxt="n", ylab=ylab, type="n", ylim=ylim, ...)
names <- unique(results$bin)
ticks <- seq(1, length(names), by=lab.sp)
if(axis.lab==TRUE) {labels <- names[ticks]} else {labels <- FALSE}
axis(1, at=ticks, labels=labels, las=2, ...)
}
poly.chron(results)
